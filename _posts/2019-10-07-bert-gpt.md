---
layout: post
title:  "BERT and GPT"
date:   2019-10-07 20:46:00 +0800
categories: Algorithm
---

## 简介

## Transformer

http://nlp.seas.harvard.edu/2018/04/03/attention.html

![png](http://nlp.seas.harvard.edu/images/the-annotated-transformer_14_0.png)

## BERT

https://www.bilibili.com/video/av54806580

### 如何训练BERT

方法一：Masked LM：对一句话中的随机单词进行遮挡，通过剩下的部分来预测这个单词。

![1570451540708](/assets/2019-10-07-bert-gpt/1570451540708.png)

方法二：Next Sentence Prediction，将两个句子用一个特殊的token [SEP]串接起来，使用输出开始的位置的向量添加线性分类器得到是否是两个连续句子的判断。

![1570451740132](/assets/2019-10-07-bert-gpt/1570451740132.png)

### BERT的使用场合

| 序号 |   输入   |            输出            |                        例子                         |
| :--: | :------: | :------------------------: | :-------------------------------------------------: |
|  1   | 单个语句 |          一个类别          |                 情感分类，文档分类                  |
|  2   | 单个语句 |       每个单词的类别       |                    Slot filling                     |
|  3   | 两个语句 |          一个类别          |             Natural Language Inference              |
|  4   | 两个语句 | 一个起始位置，一个结束位置 | Extraction-based Question Answering (QA) e.g. SQuAD |

![1570452093405](/assets/2019-10-07-bert-gpt/1570452093405.png)

![1570452056677](/assets/2019-10-07-bert-gpt/1570452056677.png)

![1570452118500](/assets/2019-10-07-bert-gpt/1570452118500.png)

![1570452236549](/assets/2019-10-07-bert-gpt/1570452236549.png)

## GPT

## 参考

TODO
