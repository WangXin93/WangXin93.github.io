---
categories: Algorithm
date: "2018-11-20T19:33:02Z"
title: 'TL-GAN: transparent latent-space GAN'
toc: true
---

## 前言

GAN提供了一种从属性到图像的转化方法，但是GAN通常是从一个随机噪声开始通过generator生成逼真的图像的，我们并不知道生成的图像会有什么样的属性，比如生成的人脸有没有胡子，是男性还是女性。如何更加有控制地生成图像？

为了实现可控制合成，人们已经创建了很多GAN的变体，大体分为两类：风格迁移网络，条件生成器。

* 风格迁移网络：以CycleGAN和pix2pix为代表，是用来将图像从一个领域迁移到另一领域（例如，从马到斑马，从素描到彩色图像）的模型。因此，我们不能在两个离散状态之间连续调整一个特征（例如，在脸上添加更多胡须）。另外，一个网络专用于一种类型的迁移，因此调整 10 个特征需要十个不同的神经网络。
* 条件生成器：以 conditional GAN，AC-GAN 和 Stack-GAN 为代表，是在训练期间联合学习带有特征标签的图像的模型，使得图像生成能够以自定义特征为条件。因此，如果你想在生成过程中添加新的可调特征，你就得重新训练整个 GAN 模型，而这将耗费大量的计算资源和时间（例如，在带有完美超参数的单一 K80 GPU 上需要几天甚至几个星期）。此外，你要用包含所有自定义特征标签的单个数据集来执行训练，而不是利用来自多个数据集的不同标签。

<div style="text-align:center">
<img src="https://github.com/SummitKwan/transparent_latent_gan/blob/master/static/online_demo_run_fast_01.gif?raw=true" style="width:60%"/>
</div>

TL-GAN提供了一种新型高效的可控合成和编辑方法

## 方法
### 1. 揭示特征轴

TL-GAN的的方法核心是利用已经训练好的生成器和分类器来构建随机噪声$$Z$$和生成图像$$G(Z)$$之间的关系。

其整体流程如下：

<div style="text-align:center">
<img src="/assets/tl_gan/framework.jpg" style="width:60%"/>
</div>

具体步骤为：
1. 学习分布：选择一个训练好的 GAN 模型作为生成器网络。我选择的是训练好的 pg-GAN，它提供的人脸生成质量最好。

2. 分类：选择一个预训练的特征提取器模型（可以是卷积神经网络，也可以是其它计算机视觉模型），或者利用标注数据集训练自己的特征提取器。我在 CelebA 数据集上训练了一个简单的 CNN，该数据集包含三万余张人脸图像，每个图像有 40 个标签。

3. 生成：生成大量随机潜在向量，并传输到训练好的 GAN 生成器中以生产合成图像，然后使用训练好的特征提取器为每张图像生成特征。

4. 关联：使用广义线性模型（Generalized Linear Model，GLM）执行潜在向量和特征之间的回归任务。回归斜率（regression slope）即特征轴。

5. 探索：从一个潜在向量开始，沿着一或多个特征轴移动，并检测对生成图像的影响。

### 2. 解除相关特征轴之间的关联

<div style="text-align:center">
<img src="/assets/tl_gan/disentangle.jpg" style="width:60%"/>
</div>

上述示例也展示了该方法的缺点：相关特征轴。举例来说，当我打算减少胡须量时，生成的人脸图像更女性化，而这并非用户期望的结果。问题在于性别特征和胡须特征天然相关，修改一个必然会导致另一个也发生改变。类似的还有发际线和卷发。如下图所示，潜在空间中原始的「胡须」特征轴不垂直于性别特征轴。

为了解决这个问题，我使用直接的线性代数 trick。具体来说，我将胡须特征轴投影到新的方向，新方向垂直于性别特征轴，这就有效去除了二者之间的关联，从而解除生成人脸图像中这两个特征的关联。

## 参考
* [repo](https://github.com/SummitKwan/transparent_latent_gan)
* [slides](https://docs.google.com/presentation/d/1OpcYLBVpUF1L-wwPHu_CyKjXqXD0oRwBoGP2peSCrSA/edit#slide=id.g43f48f6903_0_0)
* [定制人脸图像没那么难！使用TL-GAN模型轻松变脸](https://mp.weixin.qq.com/s/8aL7COItG7lS4q5-3IZCmQ)
