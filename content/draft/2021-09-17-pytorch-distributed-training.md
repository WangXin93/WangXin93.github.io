---
categories: Python Pytorch
date: "2021-09-17T13:58:00Z"
draft: true
title: Pytorch ä½¿ç”¨å¤šGPUè¿›è¡Œè®­ç»ƒ
toc: true
---

> ä½¿ç”¨å¤šå—GPUæ¥åŒæ—¶è®­ç»ƒå¯ä»¥è§£å†³å•å—GPUè®­ç»ƒæ—¶å€™å‡ºç°çš„æ˜¾å­˜ä¸è¶³æˆ–è€…è®­ç»ƒæ—¶é—´è¿‡é•¿é—®é¢˜ã€‚æœ¬ç¯‡åšå®¢ä¼šä»‹ç»å¦‚ä½•ä½¿ç”¨Pytorchçš„DataParalleæ¨¡å—æ¥è¿›è¡Œå•æœºå™¨å¤šGPUè®­ç»ƒï¼Œä½¿ç”¨DistributedDataParallelè¿›è¡Œæ›´å¿«é€Ÿçš„å•æœºå™¨å¤šGPUè®­ç»ƒï¼Œå¤šæœºå™¨å¤šGPUè®­ç»ƒï¼Œä½¿ç”¨launch scriptæ¥æ‰§è¡Œå¤šGPUè®­ç»ƒä»»åŠ¡ã€‚

## ç®€ä»‹

å•å—GPUåœ¨é¢å¯¹å¤æ‚çš„æ·±åº¦å­¦ä¹ æ¨¡å‹æ—¶ä¼šå‡ºç°æ˜¾å­˜å®¹é‡ä¸è¶³æˆ–è€…è®¡ç®—æ—¶é—´è¿‡é•¿çš„é—®é¢˜ï¼Œä½¿ç”¨å¤šå—GPUæ¥åŒæ—¶è®­ç»ƒæ˜¯è§£å†³è¿™äº›é—®é¢˜çš„æœ‰æ•ˆæ–¹æ³•ä¹‹ä¸€ã€‚Pytorchæä¾›äº†ä¸åŒçš„è½¯ä»¶åŒ…æ¥å®ç°å®ç°å¤šå—GPUæ¥è®­ç»ƒï¼Œè¿™äº›è½¯ä»¶åŒ…å¯ä»¥æŒ‰ç…§ç”±ç®€å•åˆ°å¤æ‚çš„è§„å¾‹å¦‚ä¸‹åˆ—å‡ºï¼Œå¼€å‘è€…å¯ä»¥æ ¹æ®å¼€å‘ä»»åŠ¡å’Œå¼€å‘é˜¶æ®µè¿›è¡Œé€‰æ‹©ä½¿ç”¨ï¼š

* åœ¨å•ä¸ªGPUçš„æƒ…å†µä¸‹ï¼Œæ•°æ®å’Œæ¨¡å‹éƒ½å®¹çº³åœ¨ä¸€ä¸ªGPUï¼Œè¿™é€‚åˆåº”ç”¨çš„åŸå‹å¼€å‘
* å¦‚æœä¸€ä¸ªæœºå™¨æœ‰å¤šä¸ªGPUï¼Œä½ å¸Œæœ›ä½¿ç”¨æœ€å°‘çš„ä»£ç æ”¹åŠ¨æ¥å¯ç”¨å¤šGPUæ¥åŠ é€Ÿè®­ç»ƒæˆ–è€…æ‰©å¤§æ˜¾å­˜ï¼Œå¯ä»¥ä½¿ç”¨[DataParallel](https://pytorch.org/docs/master/generated/torch.nn.DataParallel.html)
* å¦‚æœå¸Œæœ›è¿›ä¸€æ­¥æé«˜è®­ç»ƒé€Ÿåº¦å¹¶ä¸”ä¸ä»‹æ„å†å¤šå†™ç‚¹ä»£ç ï¼Œå¯ä»¥ä½¿ç”¨[DistributedDataParallel](https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html)
* å¦‚æœä½ å¸Œæœ›ä½¿ç”¨åˆ°å¤šå°æœºå™¨ï¼Œå¯ä»¥ä½¿ç”¨[DistributedDataParallel](https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html) å’Œ [launching script](https://github.com/pytorch/examples/blob/master/distributed/ddp/README.md)
* å¦‚æœå¸Œæœ›èƒ½å¤Ÿåœ¨åˆ†å¸ƒå¼è®­ç»ƒçš„æ—¶å€™å¤„ç†é”™è¯¯æˆ–è€…åœ¨è®­ç»ƒæ—¶å€™è®¡ç®—èµ„æºèƒ½å¤ŸåŠ¨æ€çš„åŠ å…¥æˆ–è€…ç¦»å¼€ï¼Œå¯ä»¥ä½¿ç”¨[torchelastic](https://pytorch.org/elastic)
* å¦‚æœè®­ç»ƒæ—¶å€™å‘ç°å…ˆæœ‰è½¯ä»¶åŒ…çš„èŒƒå¼ä¸èƒ½æ»¡è¶³è¦æ±‚ï¼Œæ¯”å¦‚å¸Œæœ›ä½¿ç”¨parameter serverèŒƒå¼ï¼Œdistributed pipeline èŒƒå¼ï¼Œreinforcement learning applications with multiple observers or agentsç­‰ï¼Œå¯ä»¥ä½¿ç”¨ [torch.distributed.rpc](https://pytorch.org/docs/master/rpc.html) æ¥å¼€å‘ä»¥æ»¡è¶³è¦æ±‚ã€‚

## torch.nn.DataParallel

``DataParallel``åŒ…è¿è¡Œå¼€èƒ½å¤Ÿä½¿ç”¨ä¸€è¡Œä»£ç å°±èƒ½å¤Ÿå®ç°ä¸€å°æœºå™¨ä¸Šçš„å¤šGPUè®­ç»ƒï¼š

```python
model = nn.DataParallel(model)
```

å¯ä»¥å‚è€ƒ[Optional: Data Parallelism](https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html)æ¥æŸ¥çœ‹ä½¿ç”¨DataParallelçš„å®Œæ•´ä»£ç ã€‚

å°½ç®¡ä»£ç éœ€è¦æ”¹åŠ¨çš„åœ°æ–¹å°‘ï¼Œä½¿ç”¨``DataParallel``åœ¨è¿è¡Œæ•ˆç‡è€ƒè™‘å´ä¸æ˜¯æœ€å¥½çš„é€‰æ‹©ã€‚è¿™æ—¶å› ä¸º``DataParallel``çš„å®ç°æ–¹æ³•ä¸­åœ¨æ¯ä¸€æ¬¡æ¨¡å‹çš„å‰å‘ä¼ æ’­éƒ½ä¼šå¤åˆ¶ä¸€æ¬¡æ¨¡å‹ï¼ŒåŒæ—¶å®ƒä½¿ç”¨single-process multi-threadå¹¶è¡Œï¼Œæ‰€ä»¥ä¼šå—åˆ°GILçš„é™åˆ¶ã€‚å¦‚æœå¸Œæœ›å–å¾—æ›´å¥½çš„æ€§èƒ½è¡¨ç°ï¼Œåº”è¯¥ä½¿ç”¨``DistributedDataParallel``ã€‚

## torch.nn.parallel.DistributedDataParallel

åœ¨ä½¿ç”¨``DistributedDataParallel``çš„æ—¶å€™ï¼Œéœ€è¦å…ˆè®¾ç½®[init_process_group](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group)ã€‚DDPä½¿ç”¨ multi-process å¹¶è¡Œï¼Œæ‰€ä»¥æ²¡æœ‰GILçš„é™åˆ¶ã€‚åŒæ—¶æ¨¡å‹å®åœ¨DDPæ„å»ºçš„æ—¶å€™ä¼ æ’­åˆ°æ¯ä¸ªè¿›ç¨‹è€Œä¸æ˜¯æ¯æ¬¡ä¼ æ’­çš„æ—¶å€™å¤åˆ¶ï¼Œè¿™ä¹Ÿèƒ½å¤ŸåŠ é€Ÿè®­ç»ƒã€‚DDPè¿˜æœ‰å…¶å®ƒä¼˜åŒ–æŠ€æœ¯ï¼Œå…·ä½“å¯ä»¥æŸ¥çœ‹[DDP paper](http://www.vldb.org/pvldb/vol13/p3005-li.pdf)ã€‚

### åŸºæœ¬åŸç†

è¦æƒ³åˆ›å»ºDDPæ¨¡å‹ï¼Œé¦–å…ˆè¦æ­£ç¡®è®¾ç½®process groupsã€‚pytorchçš„DistributedDataParallelï¼ˆDDPï¼‰æ¨¡å—éœ€è¦æŒ‡å®šæ„å»ºå¥½çš„æ·±åº¦å­¦ä¹ ç½‘ç»œæ¨¡å‹moduleç±»ï¼ŒDDPä¼šæŠŠæ¨¡å‹å’Œå…¶ä¸­çš„çŠ¶æ€ä¼ æ’­åˆ°rank0åˆ°Nä¸åŒçš„è¿›ç¨‹ä¸­ã€‚DDPç®—æ³•çš„æ„å»ºï¼Œå‰å‘ä¼ æ’­ï¼Œåå‘ä¼ æ’­æ˜¯é‡è¦çš„æ­¥éª¤ã€‚å‰å‘ä¼ æ’­ä¼šæŠŠæ•°æ®åˆ’åˆ†å–‚å…¥åˆ°ä¸åŒçš„è¿›ç¨‹å½“ä¸­ï¼Œåå‘ä¼ æ’­æ—¶å€™ä¼šæ”¶é›†ä¸åŒè¿›ç¨‹çš„æ¢¯åº¦ï¼Œç„¶åè¿›è¡Œreduceï¼Œæœ€åæŠŠæ¯ä¸ªè¿›ç¨‹ä¸­çš„æ¨¡å‹çŠ¶æ€æ ¹æ®æ¢¯åº¦è¿›è¡Œæ›´æ–°ã€‚

![img](https://miro.medium.com/max/2000/1*FpDHkWJhkLL7KxU01Lf9Lw.png)

DDPåœ¨æ„å»ºé˜¶æ®µå°±ä¼šå®Œæˆåˆå§‹åŒ–æ¨¡å‹çš„çŠ¶æ€ï¼Œä½ ä¸éœ€è¦æ‹…å¿ƒæ¯ä¸ªè¿›ç¨‹åŒ…å«ä¸åŒçš„åˆå§‹çŠ¶æ€ã€‚

DDP moduleå·²ç»æŠŠåº•å±‚çš„åˆ†å¸ƒå¼é€šä¿¡ç»†èŠ‚éšè—äº†èµ·æ¥ï¼Œåªæš´éœ²äº†ç±»ä¼¼äºå•ä¸€æœ¬åœ°æ¨¡å‹çš„APIã€‚

ä½¿ç”¨DDPçš„æ—¶å€™ï¼Œæ¨¡å‹ä¼šè¢«å…ˆsaveåˆ°ä¸€ä¸ªè¿›ç¨‹ï¼Œç„¶ååŠ è½½åˆ°æ‰€æœ‰çš„è¿›ç¨‹ã€‚æ‰€ä»¥éœ€è¦æ³¨æ„æ‰€æœ‰è¿›ç¨‹è¦åœ¨æ¨¡å‹saveå®Œæˆä¹‹ååœ¨loadã€‚åŒæ—¶åœ¨ä¿å­˜çš„æ—¶å€™ï¼ŒæŒ‡å®šä¸€ä¸ªrankè¿›è¡Œå°±è¶³å¤Ÿäº†ï¼Œåœ¨loadçš„æ—¶å€™ï¼Œéœ€è¦æŒ‡å®š``map_location``æ¥é˜²æ­¢è®¿é—®åˆ°å…¶å®ƒè®¾å¤‡ã€‚

DDPè¿˜æ”¯æŒå¤šGPUçš„æ¨¡å‹ï¼Œå³ä¸€ä¸ªæ¨¡å‹ä¸åŒæ­¥éª¤ä½¿ç”¨ä¸åŒGPUæ¥è®¡ç®—ï¼Œè¿™å¯¹äºéå¸¸å¤§çš„æ¨¡å‹è®¡ç®—å¤§é‡æ•°æ®ä¼šæœ‰ç”¨ã€‚

### æµ‹è¯•èƒ½å¦ä½¿ç”¨DDP

```python
import os
import torch.distributed as dist

print(f"[ {os.getpid()} ] Initializing process group")
dist.init_process_group(backend="nccl")
print(f"[ {os.getpid()} ] world_size = {dist.get_world_size()}, " + f"rank = {dist.get_rank()}, backend={dist.get_backend()}")
```

ä¸Šé¢çš„ä»£ç å¯ä»¥ç”¨æ¥æ£€æŸ¥åœ¨ä¸€å°æœºå™¨æˆ–è€…å¤šå°æœºå™¨èƒ½å¦æ­£å¸¸å¯åŠ¨process groupã€‚å®ƒé¦–å…ˆä½¿ç”¨``init_process_group()``å‡½æ•°å¯åŠ¨äº†process groupï¼Œç„¶åæ‰“å°å‡ºè¿›ç¨‹å·å’Œworld_sizeã€‚å¦‚æœä½ åªåœ¨ä¸€å°å¤šGPUæœºå™¨ä¸Šè¿è¡Œï¼Œéœ€è¦ä½¿ç”¨launch scriptæ‰§è¡Œç¨‹åº:

```
python -m torch.distributed.launch --nnodes 1 init_proc.py
```

å¦‚æœè¦åœ¨å¤šå°æœºå™¨ä¸Šè¿è¡Œç¨‹åºï¼Œé‚£ä¹ˆåˆ†åˆ«åœ¨æ¯ä¸ªæœºå™¨ä¸Šè¿è¡Œè„šæœ¬ï¼š

```
# first machine
python -m torch.distributed.launch --nnodes 2 --nproc_per_node 2 --node_rank 0 --master_addr="1.2.3.4" --master_port=29500 init_proc.py

# second machine
python -m torch.distributed.launch --nnodes 2 --nproc_per_node 2 --node_rank 1 --master_addr="1.2.3.4" --master_port=29500 init_proc.py
```

å…¶ä¸­``nnodes``ä¸ºæœºå™¨æ•°é‡ï¼Œ``nproc_per_node``ç­‰äºæ¯å°æœºå™¨GPUæ•°é‡ï¼Œ``node_rank``ä¸ºæ¯å°æœºå™¨çš„åºå·ï¼Œä»0åˆ°N-1ã€‚ç¨‹åºæ‰§è¡ŒæˆåŠŸåï¼Œå¯ä»¥åœ¨æ¯å°æœºå™¨ä¸Šçœ‹åˆ°ç±»ä¼¼ä¸‹é¢çš„è¾“å‡ºç»“æœã€‚

```
[ 1329560 ] Initializing process group
[ 1329560 ] world_size = 4, rank = 0, backend=nccl
```

### ä½¿ç”¨DDP spawnçš„ä¾‹å­

å¦‚æœå¸Œæœ›äº†è§£DDP spawnçš„ä½¿ç”¨æ–¹æ³•ï¼Œå¯ä»¥ç»“åˆä¸‹é¢çš„ä»£ç å¹¶[è¿™é‡Œ](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)è¿›è¡Œå‚è€ƒã€‚

```python
import os
import tempfile
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
import torch.nn as nn
import torch.optim as optim

from torch.nn.parallel import DistributedDataParallel as DDP


def setup(rank, world_size):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'

    # initialize the process group
    dist.init_process_group("gloo", rank=rank, world_size=world_size)


def cleanup():
    dist.destroy_process_group()


class ToyModel(nn.Module):
    def __init__(self):
        super(ToyModel, self).__init__()
        self.net1 = nn.Linear(10, 10)
        self.relu = nn.ReLU()
        self.net2 = nn.Linear(10, 5)

    def forward(self, x):
        return self.net2(self.relu(self.net1(x)))


def demo_basic(rank, world_size):
    print(f"Running basic DDP example on rank {rank}.")
    setup(rank, world_size)

    # create model and move it to GPU with id rank
    model = ToyModel().to(rank)
    ddp_model = DDP(model, device_ids=[rank])

    loss_fn = nn.MSELoss()
    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)

    optimizer.zero_grad()
    outputs = ddp_model(torch.randn(20, 10))
    labels = torch.randn(20, 5).to(rank)
    loss_fn(outputs, labels).backward()
    optimizer.step()

    cleanup()


def run_demo(demo_fn, world_size):
    mp.spawn(demo_fn,
             args=(world_size,),
             nprocs=world_size,
             join=True)


def demo_checkpoint(rank, world_size):
    print(f"Running DDP checkpoint example on rank {rank}.")
    setup(rank, world_size)

    model = ToyModel().to(rank)
    ddp_model = DDP(model, device_ids=[rank])

    loss_fn = nn.MSELoss()
    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)

    CHECKPOINT_PATH = tempfile.gettempdir() + "/model.checkpoint"
    if rank == 0:
        # All processes should see same parameters as they all start from same
        # random parameters and gradients are synchronized in backward passes.
        # Therefore, saving it in one process is sufficient.
        torch.save(ddp_model.state_dict(), CHECKPOINT_PATH)

    # Use a barrier() to make sure that process 1 loads the model after process
    # 0 saves it.
    dist.barrier()
    # configure map_location properly
    map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}
    ddp_model.load_state_dict(
        torch.load(CHECKPOINT_PATH, map_location=map_location))

    optimizer.zero_grad()
    outputs = ddp_model(torch.randn(20, 10))
    labels = torch.randn(20, 5).to(rank)
    loss_fn = nn.MSELoss()
    loss_fn(outputs, labels).backward()
    optimizer.step()

    # Use a barrier() to make sure that all processes have finished reading the
    # checkpoint
    dist.barrier()

    if rank == 0:
        os.remove(CHECKPOINT_PATH)

    cleanup()


class ToyMpModel(nn.Module):
    def __init__(self, dev0, dev1):
        super(ToyMpModel, self).__init__()
        self.dev0 = dev0
        self.dev1 = dev1
        self.net1 = torch.nn.Linear(10, 10).to(dev0)
        self.relu = torch.nn.ReLU()
        self.net2 = torch.nn.Linear(10, 5).to(dev1)

    def forward(self, x):
        x = x.to(self.dev0)
        x = self.relu(self.net1(x))
        x = x.to(self.dev1)
        return self.net2(x)


def demo_model_parallel(rank, world_size):
    print(f"Running DDP with model parallel example on rank {rank}.")
    setup(rank, world_size)

    # setup mp_model and devices for this process
    dev0 = rank * 2
    dev1 = rank * 2 + 1
    mp_model = ToyMpModel(dev0, dev1)
    ddp_mp_model = DDP(mp_model)

    loss_fn = nn.MSELoss()
    optimizer = optim.SGD(ddp_mp_model.parameters(), lr=0.001)

    optimizer.zero_grad()
    # outputs will be on dev1
    outputs = ddp_mp_model(torch.randn(20, 10))
    labels = torch.randn(20, 5).to(dev1)
    loss_fn(outputs, labels).backward()
    optimizer.step()

    cleanup()


if __name__ == "__main__":
    n_gpus = torch.cuda.device_count()
    if n_gpus < 2:
        print(f"Requires at least 8 GPUs to run, but got {n_gpus}.")
    else:
        run_demo(demo_basic, 2)
        run_demo(demo_checkpoint, 2)
        run_demo(demo_model_parallel, 1)
```

### ä½¿ç”¨launch script

DDPå¯ä»¥ä½¿ç”¨spawnå‡½æ•°å­µåŒ–å‡ºå¤šä¸ªè¿›ç¨‹æ¥è¿›è¡Œå¤šGPUè®­ç»ƒè¦æ±‚è®­ç»ƒçš„å‡½æ•°å¿…é¡»èƒ½å¤Ÿè¢«pickleå‹ç¼©ã€‚è€Œä½¿ç”¨launch scriptå´æ²¡æœ‰è¿™ä¸ªé™åˆ¶ï¼Œå› æ­¤ä½¿ç”¨å¾—æ›´åŠ å¹¿æ³›ã€‚

* ä½¿ç”¨``launch``è„šæœ¬éœ€è¦ä¿ç•™``--local_rank``å‘½ä»¤è¡Œå‚æ•°
* ä½¿ç”¨``DistributedDataParallel``åŒ…è£¹æ¨¡å‹
* ä½¿ç”¨``DistributedSampler``ç”¨äºdataloader
* ä¸‹è½½æ•°æ®é›†æˆ–è€…åˆ›å»ºæ–‡ä»¶å¤¹è¿™æ ·çš„æ“ä½œåœ¨åˆ†å¸ƒå¼ç¨‹åºä¸­æ˜¯ä¸å®‰å…¨çš„ï¼Œå¦‚æœä¸€å®šè¦ä½¿ç”¨ï¼Œå¯ä»¥è€ƒè™‘[`torch.distributed.barrier`](https://pytorch.org/docs/stable/distributed.html#torch.distributed.barrier)

ä¸ŠèŠ‚çš„ä»£ç å¯ä»¥æ”¹å†™æˆ:

```python
import argparse
import os
import sys
import tempfile
from urllib.parse import urlparse

import torch
import torch.distributed as dist
import torch.nn as nn
import torch.optim as optim

from torch.nn.parallel import DistributedDataParallel as DDP

class ToyModel(nn.Module):
    def __init__(self):
        super(ToyModel, self).__init__()
        self.net1 = nn.Linear(10, 10)
        self.relu = nn.ReLU()
        self.net2 = nn.Linear(10, 5)

    def forward(self, x):
        return self.net2(self.relu(self.net1(x)))


def demo_basic(local_world_size, local_rank):

    # setup devices for this process. For local_world_size = 2, num_gpus = 8,
    # rank 0 uses GPUs [0, 1, 2, 3] and
    # rank 1 uses GPUs [4, 5, 6, 7].
    n = torch.cuda.device_count() // local_world_size
    device_ids = list(range(local_rank * n, (local_rank + 1) * n))

    print(
        f"[{os.getpid()}] rank = {dist.get_rank()}, "
        + f"world_size = {dist.get_world_size()}, n = {n}, device_ids = {device_ids} \n", end=''
    )

    model = ToyModel().cuda(device_ids[0])
    ddp_model = DDP(model, device_ids)

    loss_fn = nn.MSELoss()
    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)

    optimizer.zero_grad()
    outputs = ddp_model(torch.randn(20, 10))
    labels = torch.randn(20, 5).to(device_ids[0])
    loss_fn(outputs, labels).backward()
    optimizer.step()


def spmd_main(local_world_size, local_rank):
    # These are the parameters used to initialize the process group
    env_dict = {
        key: os.environ[key]
        for key in ("MASTER_ADDR", "MASTER_PORT", "RANK", "WORLD_SIZE")
    }
    
    if sys.platform == "win32":
        # Distributed package only covers collective communications with Gloo
        # backend and FileStore on Windows platform. Set init_method parameter
        # in init_process_group to a local file.
        if "INIT_METHOD" in os.environ.keys():
            print(f"init_method is {os.environ['INIT_METHOD']}")
            url_obj = urlparse(os.environ["INIT_METHOD"])
            if url_obj.scheme.lower() != "file":
                raise ValueError("Windows only supports FileStore")
            else:
                init_method = os.environ["INIT_METHOD"]
        else:
            # It is a example application, For convience, we create a file in temp dir.
            temp_dir = tempfile.gettempdir()
            init_method = f"file:///{os.path.join(temp_dir, 'ddp_example')}"
        dist.init_process_group(backend="gloo", init_method=init_method, rank=int(env_dict["RANK"]), world_size=int(env_dict["WORLD_SIZE"]))
    else:
        print(f"[{os.getpid()}] Initializing process group with: {env_dict}")  
        dist.init_process_group(backend="nccl")

    print(
        f"[{os.getpid()}]: world_size = {dist.get_world_size()}, "
        + f"rank = {dist.get_rank()}, backend={dist.get_backend()} \n", end=''
    )

    demo_basic(local_world_size, local_rank)

    # Tear down the process group
    dist.destroy_process_group()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    # This is passed in via launch.py
    parser.add_argument("--local_rank", type=int, default=0)
    # This needs to be explicitly passed in
    parser.add_argument("--local_world_size", type=int, default=1)
    args = parser.parse_args()
    # The main entry point is called directly without using subprocess
    spmd_main(args.local_world_size, args.local_rank)
```

è¿è¡Œç¨‹åºçš„å‘½ä»¤ä¸ºï¼š

```
python /path/to/launch.py --nnode=1 --node_rank=0 --nproc_per_node=8 example.py --local_world_size=8

python /path/to/launch.py --nnode=1 --node_rank=0 --nproc_per_node=1 example.py --local_world_size=1
```

### ä¸€ä¸ªDDPå®é™…ä½¿ç”¨ä¾‹å­

ä¸‹é¢çš„ä»£ç æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨``torch.distributed.launch``æ¥åˆ†å¸ƒå¼è®­ç»ƒCIFAR-10åˆ†ç±»ä»»åŠ¡ï¼š

```python
import torch
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data import DataLoader
import torch.nn as nn
import torch.optim as optim

import torchvision
import torchvision.transforms as transforms

import argparse
import os
import random
import numpy as np

def set_random_seeds(random_seed=0):

    torch.manual_seed(random_seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    np.random.seed(random_seed)
    random.seed(random_seed)

def evaluate(model, device, test_loader):

    model.eval()

    correct = 0
    total = 0
    with torch.no_grad():
        for data in test_loader:
            images, labels = data[0].to(device), data[1].to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    accuracy = correct / total

    return accuracy

def main():

    num_epochs_default = 10000
    batch_size_default = 256 # 1024
    learning_rate_default = 0.1
    random_seed_default = 0
    model_dir_default = "saved_models"
    model_filename_default = "resnet_distributed.pth"

    # Each process runs on 1 GPU device specified by the local_rank argument.
    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument("--local_rank", type=int, help="Local rank. Necessary for using the torch.distributed.launch utility.")
    parser.add_argument("--num_epochs", type=int, help="Number of training epochs.", default=num_epochs_default)
    parser.add_argument("--batch_size", type=int, help="Training batch size for one process.", default=batch_size_default)
    parser.add_argument("--learning_rate", type=float, help="Learning rate.", default=learning_rate_default)
    parser.add_argument("--random_seed", type=int, help="Random seed.", default=random_seed_default)
    parser.add_argument("--model_dir", type=str, help="Directory for saving models.", default=model_dir_default)
    parser.add_argument("--model_filename", type=str, help="Model filename.", default=model_filename_default)
    parser.add_argument("--resume", action="store_true", help="Resume training from saved checkpoint.")
    argv = parser.parse_args()

    local_rank = argv.local_rank
    num_epochs = argv.num_epochs
    batch_size = argv.batch_size
    learning_rate = argv.learning_rate
    random_seed = argv.random_seed
    model_dir = argv.model_dir
    model_filename = argv.model_filename
    resume = argv.resume

    # Create directories outside the PyTorch program
    # Do not create directory here because it is not multiprocess safe
    '''
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)
    '''

    model_filepath = os.path.join(model_dir, model_filename)

    # We need to use seeds to make sure that the models initialized in different processes are the same
    set_random_seeds(random_seed=random_seed)

    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs
    torch.distributed.init_process_group(backend="nccl")
    # torch.distributed.init_process_group(backend="gloo")

    # Encapsulate the model on the GPU assigned to the current process
    model = torchvision.models.resnet18(pretrained=False)

    device = torch.device("cuda:{}".format(local_rank))
    model = model.to(device)
    ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)

    # We only save the model who uses device "cuda:0"
    # To resume, the device for the saved model would also be "cuda:0"
    if resume == True:
        map_location = {"cuda:0": "cuda:{}".format(local_rank)}
        ddp_model.load_state_dict(torch.load(model_filepath, map_location=map_location))

    # Prepare dataset and dataloader
    transform = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])

    # Data should be prefetched
    # Download should be set to be False, because it is not multiprocess safe
    train_set = torchvision.datasets.CIFAR10(root="data", train=True, download=False, transform=transform) 
    test_set = torchvision.datasets.CIFAR10(root="data", train=False, download=False, transform=transform)

    # Restricts data loading to a subset of the dataset exclusive to the current process
    train_sampler = DistributedSampler(dataset=train_set)

    train_loader = DataLoader(dataset=train_set, batch_size=batch_size, sampler=train_sampler, num_workers=8)
    # Test loader does not have to follow distributed sampling strategy
    test_loader = DataLoader(dataset=test_set, batch_size=128, shuffle=False, num_workers=8)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(ddp_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-5)

    # Loop over the dataset multiple times
    for epoch in range(num_epochs):

        print("Local Rank: {}, Epoch: {}, Training ...".format(local_rank, epoch))
        
        # Save and evaluate model routinely
        if epoch % 10 == 0:
            if local_rank == 0:
                accuracy = evaluate(model=ddp_model, device=device, test_loader=test_loader)
                torch.save(ddp_model.state_dict(), model_filepath)
                print("-" * 75)
                print("Epoch: {}, Accuracy: {}".format(epoch, accuracy))
                print("-" * 75)

        ddp_model.train()

        for data in train_loader:
            inputs, labels = data[0].to(device), data[1].to(device)
            optimizer.zero_grad()
            outputs = ddp_model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

if __name__ == "__main__":
    
    main()
```

åœ¨ç¬¬ä¸€ä¸ªæœºå™¨ä¸Šè¿è¡Œï¼š

```
python -m torch.distributed.launch --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr="192.168.0.1" --master_port=1234 resnet_ddp.py
```

åœ¨ç¬¬äºŒä¸ªæœºå™¨ä¸Šè¿è¡Œï¼š

```
python -m torch.distributed.launch --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr="192.168.0.1" --master_port=1234 resnet_ddp.py
```

### å¸¸è§é—®é¢˜

* ç¡®è®¤æœºå™¨å®‰è£…çš„pytorchç‰ˆæœ¬ä¸€æ ·ï¼Œæœ€å¥½éƒ½æ˜¯æœ€è¿‘çš„ç‰ˆæœ¬ï¼Œæˆ‘è¯•è¿‡pytorch1.7å’Œpytorch1.8ä¹‹é—´æ˜¯ä¸èƒ½é€šä¿¡çš„
* ç¡®è®¤æ¯ä¸ªæœºå™¨ä¸Šçš„é˜²ç«å¢™å·²ç»æ‰“å¼€ï¼Œå¯ä»¥ä½¿ç”¨telnelæ¥æ£€æŸ¥å¦ä¸€å°æœºå™¨çš„ç«¯å£æ˜¯å¦èƒ½å¤Ÿè¿é€šï¼Œä½ å¯ä»¥å‚è€ƒ[init_process_group with launch.py --nnode=2 hangs always in all machines Â· Issue #52848 Â· pytorch/pytorch (github.com)](https://github.com/pytorch/pytorch/issues/52848)æ¥debugæœºå™¨çš„ç½‘ç»œ

### å•æœºå™¨å¤šGPUè®­ç»ƒ

```
python -m torch.distributed.launch --nproc_per_node=4
--nnodes=1 --node_rank=0
--master_port=1234 train.py <OTHER TRAINING ARGS>
```

### ç»“æŸè®­ç»ƒ

```
kill $(ps aux | grep resnet_ddp.py | grep -v grep | awk '{print $2}')
```

## å‚è€ƒ

* [PyTorch Distributed Overview â€” PyTorch Tutorials 1.9.0+cu102 documentation](https://pytorch.org/tutorials/beginner/dist_overview.html)
* [examples/distributed/ddp at master Â· pytorch/examples (github.com)](https://github.com/pytorch/examples/tree/master/distributed/ddp)
* [Lei Mao's Log Book â€“ PyTorch Distributed Training](https://leimao.github.io/blog/PyTorch-Distributed-Training/)
* [Invited Talk: PyTorch Distributed (DDP, RPC) - By Facebook Research Scientist Shen Li - YouTube](https://www.youtube.com/watch?v=3XUG7cjte2U)
* [examples/main.py at master Â· pytorch/examples (github.com)](https://github.com/pytorch/examples/blob/master/imagenet/main.py)
* [ğŸ’¥ Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU & Distributed setups | by Thomas Wolf | HuggingFace | Medium](https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255)
* [Distributed Neural Network Training In Pytorch | by Nilesh Vijayrania | Towards Data Science](https://towardsdatascience.com/distributed-neural-network-training-in-pytorch-5e766e2a9e62)